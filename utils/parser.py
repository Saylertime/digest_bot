import requests
from bs4 import BeautifulSoup
import os
import random
from urllib.parse import urljoin, urlsplit, urlunsplit, urlparse
import feedparser
import certifi
import re
from loader import bot
from pg_maker import all_users
from aiogram.types import FSInputFile


HEADERS = {"User-Agent": "Mozilla/5.0"}


def load_seen_links(file_name):
    if not os.path.exists(file_name):
        return set()
    with open(file_name, "r", encoding="utf-8") as f:
        return set(
            line.strip().rsplit(" ‚Äî ", 1)[-1]
            for line in f if " ‚Äî " in line
        )


def save_seen_links(entries, file_name):
    with open(file_name, "a", encoding="utf-8") as f:
        for title, link, source in entries:
            f.write(f"{title} ‚Äî {link}\n")


async def fetch_sostav():
    URL = "https://www.sostav.ru/news/digital"
    BASE_URL = "https://www.sostav.ru"
    SEEN_FILE_SOSTAV = "links_news.txt"

    response = requests.get(URL, headers=HEADERS)
    soup = BeautifulSoup(response.text, "html.parser")

    seen_links = load_seen_links(SEEN_FILE_SOSTAV)
    new_articles = []

    for news in soup.find_all("a", class_="title"):
        title = news.text.strip()
        relative_link = news.get("href", "")
        full_link = BASE_URL + relative_link

        if full_link not in seen_links:
            new_articles.append((title, full_link, "Sostav"))

    save_seen_links(new_articles, SEEN_FILE_SOSTAV)
    return new_articles


async def fetch_vc():
    URL = "https://vc.ru/design"
    BASE_URL = "https://vc.ru"
    SEEN_FILE_VC = "links.txt"

    response = requests.get(URL, headers=HEADERS)
    soup = BeautifulSoup(response.text, "html.parser")
    seen_links = load_seen_links(SEEN_FILE_VC)

    new_articles = []

    for article in soup.find_all("div", class_="content content--short"):
        title_block = article.find("div", class_="content-title")
        title = title_block.get_text(strip=True) if title_block else "–ù–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞"

        link_tag = article.find("a", class_="content__link")
        href = link_tag.get("href") if link_tag else ""
        full_link = urljoin(BASE_URL, href) if href else ""

        if not full_link:
            continue

        if full_link not in seen_links:
            new_articles.append((title, full_link, "VC"))

    save_seen_links(new_articles, SEEN_FILE_VC)
    return new_articles


async def fetch_habr():
    rss_url = "https://habr.com/ru/rss/flows/design/articles/?fl=ru"
    SEEN_FILE_HABR = "links.txt"

    seen_links = load_seen_links(SEEN_FILE_HABR)
    response = requests.get(rss_url, verify=certifi.where())
    feed = feedparser.parse(response.content)

    new_articles = []

    for entry in feed.entries:
        title = entry.title.strip()
        link = entry.link.strip()

        if link not in seen_links:
            new_articles.append((title, link, "Habr"))

    save_seen_links(new_articles, SEEN_FILE_HABR)
    return new_articles

_ARTICLE_RX = re.compile(r"^https?://dsgners\.ru/[^/]+/\d+-", re.I)

def _is_article_link(url: str) -> bool:
    return bool(_ARTICLE_RX.match(url))

def _strip_fragment(url: str) -> str:
    # —É–±–∏—Ä–∞–µ–º #comments –∏ –ø—Ä–æ—á–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
    parts = list(urlsplit(url))
    parts[3] = ""  # query –Ω–µ —Ç—Ä–æ–≥–∞–µ–º
    parts[4] = ""  # fragment
    return urlunsplit(parts)

def _extract_clean_title(a_tag) -> str:
    """
    –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫:
    1) —Ç–∏–ø–∏—á–Ω—ã–µ <span class="line-clamp-3"> –≤–Ω—É—Ç—Ä–∏ —Å—Å—ã–ª–∫–∏
    2) aria-label / title —É <a>
    3) –ø—Ä—è–º–æ–π —Ç–µ–∫—Å—Ç —É–∑–ª–∞ <a> (–±–µ–∑ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Å—á–µ—Ç—á–∏–∫–æ–≤)
    4) –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç: —Å–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –≤–Ω—É—Ç—Ä–∏ <a>
    """
    # 1) —Å–∞–º—ã–π —á–∞—Å—Ç—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –Ω–∞ dsgners.ru
    span = a_tag.select_one("span.line-clamp-3") or a_tag.select_one("span.h2") \
           or a_tag.select_one("span.group-hover\\:text-text-tertiary") \
           or a_tag.select_one("span.group-hover\\:text-text-primary")
    if span:
        t = span.get_text(" ", strip=True)
        if t:
            return t

    # 2) aria-label / title
    for attr in ("aria-label", "title"):
        if a_tag.has_attr(attr):
            t = (a_tag.get(attr) or "").strip()
            if t:
                return t

    # 3) –ø—Ä—è–º–æ–π —Ç–µ–∫—Å—Ç <a> (–±–µ–∑ –¥–æ—á–µ—Ä–Ω–∏—Ö —É–∑–ª–æ–≤)
    direct = "".join(a_tag.find_all(string=True, recursive=False)).strip()
    if direct:
        return direct

    # 4) –∑–∞–ø–∞—Å–Ω–æ–π ‚Äî —Å–∞–º—ã–π —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
    texts = [s.strip() for s in a_tag.stripped_strings if s and len(s.strip()) > 2]
    return max(texts, key=len) if texts else ""


async def fetch_dsgners(articles=True):
    URL = "https://dsgners.ru/" if articles else "https://dsgners.ru/news"
    BASE = "https://dsgners.ru/"
    SEEN_FILE = "links.txt" if articles else "links_news.txt"

    resp = requests.get(URL, headers=HEADERS, timeout=15)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")

    seen = load_seen_links(SEEN_FILE)
    out, seen_now = [], set()

    # 1) –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏
    for a in soup.select("article a[href]"):
        href = a["href"].strip()
        link = urljoin(BASE, href)
        link = _strip_fragment(link)
        if not _is_article_link(link):
            continue

        title = _extract_clean_title(a)
        if not title:
            continue

        if link in seen_now:
            continue
        seen_now.add(link)

        if link not in seen:
            out.append((title, link, "DSGNERS"))

    # 2) –ø–æ–¥—Å—Ç—Ä–∞—Ö–æ–≤–∫–∞ (–µ—Å–ª–∏ –≤–¥—Ä—É–≥ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏ –Ω–∞ —Ç–µ–∫—É—â–µ–π —Ä–∞–∑–º–µ—Ç–∫–µ)
    if not out:
        for a in soup.select("a[href]"):
            href = a["href"].strip()
            link = urljoin(BASE, href)
            link = _strip_fragment(link)
            if not _is_article_link(link):
                continue

            title = _extract_clean_title(a)
            if not title:
                continue

            if link in seen_now:
                continue
            seen_now.add(link)

            if link not in seen:
                out.append((title, link, "DSGNERS"))

    save_seen_links(out, SEEN_FILE)
    return out

# async def fetch_all():
#     sostav = await fetch_sostav()
#     vc = await fetch_vc()
#     habr = await fetch_habr()
#
#     all_articles = sostav + vc + habr
#     all_users_ids = await all_users()
#     user_ids_str = [str(record["telegram_id"]) for record in all_users_ids]
#
#     parts = []
#     current_part = ""
#
#     for title, link, source in all_articles:
#         html_line = f'<a href="{link}"><b>{title}</b></a> ‚Äî {source}\n\n'
#         if len(current_part) + len(html_line) <= 4000:
#             current_part += html_line
#         else:
#             parts.append(current_part.strip())
#             current_part = html_line
#
#     if current_part:
#         parts.append(current_part.strip())
#     for user_id in user_ids_str:
#         for part in parts:
#             try:
#                 await bot.send_message(str(user_id), str(part), parse_mode="HTML")
#             except Exception as e:
#                 await bot.send_message("68086662", str(e))


ARTICLES_FILE = "links.txt"
NEWS_FILE = "links_news.txt"
SENT_ART_FILE = "sent_articles.txt"
SENT_NEWS_FILE = "sent_news.txt"

def _read_archive(path):
    """–ß–∏—Ç–∞–µ—Ç –∞—Ä—Ö–∏–≤: —Å—Ç—Ä–æ–∫–∏ –≤–∏–¥–∞ 'Title ‚Äî https://link' -> [(title, link), ...]."""
    if not os.path.exists(path):
        return []
    out = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if " ‚Äî " not in line:
                continue
            title, link = line.rsplit(" ‚Äî ", 1)
            title = title.strip().strip("[]").strip()
            link = link.strip()
            if title and link:
                out.append((title, link))
    # —É–±–µ—Ä—ë–º –¥—É–±–ª–∏ –ø–æ —Å—Å—ã–ª–∫–µ (–æ—Å—Ç–∞–≤–ª—è—è –ø–µ—Ä–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫)
    seen = set()
    uniq = []
    for t, l in out:
        if l not in seen:
            uniq.append((t, l))
            seen.add(l)
    return uniq

def _read_sent_set(path: str) -> set[str]:
    """–ß–∏—Ç–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ (–ø–æ —Å—Å—ã–ª–∫–µ)."""
    if not os.path.exists(path):
        return set()
    sent = set()
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if " ‚Äî " in line:
                _, link = line.rsplit(" ‚Äî ", 1)
                sent.add(link.strip())
    return sent

def _append_sent(path: str, chosen: list[tuple[str, str]]) -> None:
    """–î–æ–±–∞–≤–ª—è–µ—Ç –≤—ã–±—Ä–∞–Ω–Ω—ã–µ (title, link) –≤ –∏—Å—Ç–æ—Ä–∏—é –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö."""
    if not chosen:
        return
    with open(path, "a", encoding="utf-8") as f:
        for title, link in chosen:
            f.write(f"{title} ‚Äî {link}\n")

def _source_from_url(link: str) -> str:
    host = urlparse(link).netloc.lower()
    if "vc.ru" in host:       return "VC"
    if "habr.com" in host:    return "Habr"
    if "sostav.ru" in host:   return "Sostav"
    if "dsgners.ru" in host:  return "Dsgners"
    return host or "web"

def _pick_random_without_repeats(archive_path: str, sent_path: str, n: int) -> list[tuple[str, str, str]]:
    """
    –ë–µ—Ä—ë–º –∏–∑ –∞—Ä—Ö–∏–≤–∞ N —Å–ª—É—á–∞–π–Ω—ã—Ö (title, link, source), –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏–∏.
    –ï—Å–ª–∏ unseen < N ‚Äî –≤–µ—Ä–Ω—ë–º —Å–∫–æ–ª—å–∫–æ –µ—Å—Ç—å (–ø–æ–≤—Ç–æ—Ä–æ–≤ –Ω–µ –±—É–¥–µ—Ç).
    """
    all_items = _read_archive(archive_path)
    sent = _read_sent_set(sent_path)
    unseen = [(t, l) for (t, l) in all_items if l not in sent]

    if not unseen:
        return []

    if len(unseen) > n:
        unseen = random.sample(unseen, n)

    # –¥–æ–±–∞–≤–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫
    return [(t, l, _source_from_url(l)) for (t, l) in unseen]


def _safe_remove(path: str):
    try:
        if os.path.exists(path):
            os.remove(path)
    except Exception:
        # –Ω–µ –ø–∞–¥–∞–µ–º –∏–∑-–∑–∞ —Ñ–∞–π–ª–æ–≤–æ–π –æ—à–∏–±–∫–∏
        pass

# ---------- —Å–±–æ—Ä –¥–∞–π–¥–∂–µ—Å—Ç–∞ ----------
def build_daily_digest(n_each: int = 5) -> tuple[str, list[tuple[str,str]], list[tuple[str,str]]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      - –≥–æ—Ç–æ–≤—ã–π HTML-—Ç–µ–∫—Å—Ç,
      - —Å–ø–∏—Å–æ–∫ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π [(title, link), ...],
      - —Å–ø–∏—Å–æ–∫ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π [(title, link), ...]
    –≠—Ç–∏ —Å–ø–∏—Å–∫–∏ –Ω—É–∂–Ω—ã, —á—Ç–æ–±—ã –∑–∞–ø–∏—Å–∞—Ç—å –∏—Ö –≤ –∏—Å—Ç–æ—Ä–∏—é –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –ü–û–°–õ–ï —É—Å–ø–µ—à–Ω–æ–π –æ—Ç–ø—Ä–∞–≤–∫–∏.
    """
    arts  = _pick_random_without_repeats(ARTICLES_FILE, SENT_ART_FILE, n_each)
    news  = _pick_random_without_repeats(NEWS_FILE,     SENT_NEWS_FILE, n_each)

    parts = []
    parts.append("‚úèÔ∏è <b>–°—Ç–∞—Ç—å–∏</b>\n")
    if arts:
        for i, (title, link, source) in enumerate(arts, 1):
            parts.append(f'{i}. <a href="{link}">{title}</a> ‚Äî {source}')
    else:
        parts.append("–ù–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –ø–æ–∫–∞ –Ω–µ—Ç.")

    parts.append("\n\nüì∞ <b>–ù–æ–≤–æ—Å—Ç–∏</b>\n")
    if news:
        for i, (title, link, source) in enumerate(news, 1):
            parts.append(f'{i}. <a href="{link}">{title}</a> ‚Äî {source}')
    else:
        parts.append("–ù–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ–∫–∞ –Ω–µ—Ç.")

    html = "\n".join(parts)
    html += "\n\n#DigitalDigest"
    # –≤–µ—Ä–Ω—ë–º –ø–∞—Ä—ã –±–µ–∑ source, —á—Ç–æ–±—ã –ø–∏—Å–∞—Ç—å –≤ –∏—Å—Ç–æ—Ä–∏—é
    chosen_articles = [(t, l) for (t, l, _) in arts]
    chosen_news = [(t, l) for (t, l, _) in news]
    return html, chosen_articles, chosen_news

# ---------- –µ–∂–µ–¥–Ω–µ–≤–Ω–∞—è –∑–∞–¥–∞—á–∞ ----------
async def send_daily_digest(bot, chat_ids, n_each=5):
    """
    1) –æ–±–Ω–æ–≤–ª—è–µ–º –∞—Ä—Ö–∏–≤—ã –ø–∞—Ä—Å–µ—Ä–æ–º (—Ç–≤–æ–∏ —Ñ—É–Ω–∫—Ü–∏–∏), —á—Ç–æ–±—ã –ø–æ–ø–æ–ª–Ω—è–ª–∏—Å—å links*.txt
    2) —Å–æ–±–∏—Ä–∞–µ–º –¥–∞–π–¥–∂–µ—Å—Ç –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–æ–≤
    3) —à–ª—ë–º –≤—Å–µ–º chat_ids
    4) –ø—Ä–∏ —É—Å–ø–µ—Ö–µ –¥–æ–ø–∏—Å—ã–≤–∞–µ–º –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ –∏—Å—Ç–æ—Ä–∏—é
    """
    try:
        await fetch_sostav()
        await fetch_vc()
        await fetch_habr()
        await fetch_dsgners(articles=True)
        await fetch_dsgners(articles=False)
    except Exception:
        # –µ—Å–ª–∏ –ø–∞—Ä—Å–∏–Ω–≥ —É–ø–∞–ª ‚Äî –≤—Å—ë —Ä–∞–≤–Ω–æ –ø–æ–ø—Ä–æ–±—É–µ–º —Å–æ–±—Ä–∞—Ç—å –∏–∑ —É–∂–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        pass

    html, chosen_articles, chosen_news = build_daily_digest(n_each=n_each)

    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    image_path = os.path.join(BASE_DIR, "bot_cover.png")
    print(image_path)

    for chat_id in chat_ids:
        try:
            if os.path.exists(image_path):
                photo = FSInputFile(image_path)
                await bot.send_photo(
                    chat_id,
                    photo=photo,
                    caption=html,
                    parse_mode="HTML"
                )
            else:
                await bot.send_message(chat_id, html, parse_mode="HTML", disable_web_page_preview=True)

        except Exception as e:
            await bot.send_message("68086662", str(e))
            continue

    # 4) —Ñ–∏–∫—Å–∏—Ä—É–µ–º ¬´—É–∂–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ¬ª, —á—Ç–æ–±—ã –Ω–µ –ø–æ–≤—Ç–æ—Ä—è—Ç—å –≤ –±—É–¥—É—â–µ–º
    _append_sent(SENT_ART_FILE, chosen_articles)
    _append_sent(SENT_NEWS_FILE, chosen_news)

    _safe_remove(ARTICLES_FILE)
    _safe_remove(NEWS_FILE)


async def daily_digest_job(n_each: int = 5):
    """
    –î–æ—Å—Ç–∞—ë—Ç chat_ids –∏–∑ –ë–î –∏ —à–ª—ë—Ç –¥–∞–π–¥–∂–µ—Å—Ç –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–æ–≤.
    """
    try:
        rows = await all_users()  # –æ–∂–∏–¥–∞–µ—Ç—Å—è —Å–ø–∏—Å–æ–∫ dict —Å –∫–ª—é—á–æ–º "telegram_id"
        # –°–æ–±–µ—Ä—ë–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏ –≤–∞–ª–∏–¥–Ω—ã–µ chat_id
        chat_ids = []
        seen = set()
        for r in rows:
            tid = r.get("telegram_id")
            if not tid:
                continue
            # –≤ —Ç–≤–æ—ë–º –∫–æ–¥–µ –º–µ—Å—Ç–∞–º–∏ —Å—Ç—Ä–æ–∫–∏ ‚Äî –ø—Ä–∏–≤–µ–¥—ë–º –∫ int/str, –¢–µ–ª–µ–≥—Ä–∞–º —Ç–µ—Ä–ø–∏—Ç –æ–±–∞
            tid_str = str(tid).strip()
            if not tid_str or tid_str in seen:
                continue
            seen.add(tid_str)
            chat_ids.append(tid_str)

        if not chat_ids:
            return
        await send_daily_digest(bot, chat_ids, n_each=n_each)
    except Exception as e:
        print(e)